# 🧪 ドロップアウト適用実験ログ

> **目的**  
> 70 Epoch付近で確認された過学習（overfitting）を緩和するため、**各レイヤー間に Dropout レイヤー**を挿入して再学習を実施。

---

## 1. 実験概要

| 項目 | 内容 |
| ---- | ---- |
| **基準モデル** | `25-06-24_prototype` |
| **変更点** | すべての LSTM 層の後に `Dropout(p)` を追加 |
| **評価指標** | `loss`, `val_loss` (MSE) |

---

## 2. 変更点の詳細

* **Dropout 率** : *推定 p = 0.4*  
* **適用位置** : 各 LSTM 層と Dense 層の間  
* **その他のハイパーパラメータ** : 既存と同一  

---

## 3. 結果概要

| 区分 | 過学習 (Epoch) | `loss` の推移 | `val_loss` の推移 |
| ---- | ------------- | ------------- | ----------------- |
| **Before** | ≈ **70 Epoch** | 100 Epoch 以降 *横ばい* | 急上昇 → 過学習 |
| **After (Dropout)** | ≈ **100 Epoch** | **継続的に低下** | 横ばい → 急上昇（依然として過学習） |

---

## 4. 観察された特徴

1. **過学習の遅延**  Dropout 適用後、過学習の発生時点が約 **30 Epoch** 後ろへシフト。  
2. **Loss 曲線の改善**  Dropout 前は 0.005 付近で *停滞*、適用後は **一貫して減少**。  
3. **Val_Loss の課題**  `val_loss` は依然として **横ばい → 急上昇** パターン ➜ *過学習は完全には解消されず*。  

---

## 5. 今後の課題 📌

* `val_loss` も過学習なく低下させる追加手法を模索  
* Dropout 適用後でも `val_loss` が横ばいのまま過学習に至る原因を究明  

---

## 6. グラフ比較

### 🔹 ドロップアウト **適用前** (loss: ピンク, val_loss: イエロー)

![Loss & Val_Loss (Before)](../images/25-06-24_prototype.png)

### 🔸 ドロップアウト **適用後** (loss: グリーン, val_loss: オレンジ)

![Loss & Val_Loss (After Dropout)](../images/25-06-25_dropout.png)

---
